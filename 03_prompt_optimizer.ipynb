{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplo de código de optimizador de prompts\n",
    "Este código muestra cómo optimizar un prompt para un modelo de lenguaje GPT-3.5, utilizando GPT-4 como optimizador. El codigo esta basado en el paper [Large Language Models as Optimizers. Yang et al. (2023).](https://doi.org/10.48550/arXiv.2309.03409)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.- Setup inicial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1- Instalar librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install openai\n",
    "#! pip install tenacity\n",
    "#! pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.- Cargar librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import openai\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.- Variables de entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load secrets and config from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# OpenAI API\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "embedding_model = os.getenv(\"OPENAI_EMBEDDING_MODEL\")\n",
    "print(\"OpenAI API key: {}\".format(openai.api_key[:5] + '...' + openai.api_key[-5:]))\n",
    "\n",
    "# Model endpoint names\n",
    "gpt35_model = os.getenv(\"OPENAI_GPT35_MODEL\")\n",
    "gpt35_16k_model = os.getenv(\"OPENAI_GPT35_16K_MODEL\")\n",
    "gpt4_model = os.getenv(\"OPENAI_GPT4_MODEL\")\n",
    "print(\"GPT-3.5-Turbo model: {}\".format(gpt35_model))\n",
    "print(\"GPT-3.5-Turbo-16k model: {}\".format(gpt35_16k_model))\n",
    "print(\"GPT-4 model: {}\".format(gpt4_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.- Clase para logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger:\n",
    "    def __init__(self, log_file=None, log_dir='./logs'):\n",
    "        if not os.path.exists(log_dir):\n",
    "            os.makedirs(log_dir)\n",
    "        \n",
    "        if log_file is None:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "            log_file = f\"log_{timestamp}.log\"\n",
    "        \n",
    "        self.log_file = os.path.join(log_dir, log_file)\n",
    "\n",
    "    def _write_log(self, level, msg, verbose):\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        log_entry = f\"{timestamp} [{level}] {msg}\\n\"\n",
    "        with open(self.log_file, \"a\") as f:\n",
    "            f.write(log_entry)\n",
    "        if verbose:\n",
    "            print(log_entry.strip())\n",
    "\n",
    "    def info(self, msg, verbose=False):\n",
    "        self._write_log(\"INFO\", msg, verbose)\n",
    "\n",
    "    def warn(self, msg, verbose=False):\n",
    "        self._write_log(\"WARN\", msg, verbose)\n",
    "\n",
    "    def error(self, msg, verbose=True):\n",
    "        self._write_log(\"ERROR\", msg, verbose)\n",
    "        \n",
    "logger = Logger()\n",
    "logger.info(\"Logger initialized\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.- Clase para generación de texto con modelos GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(3))\n",
    "def generate_text(prompt, model=gpt35_model, messages=[], max_tokens=100, temperature=1.0, top_p=1.0, frequency_penalty=0.0, presence_penalty=0.0, stop=None):\n",
    "    _messages = []\n",
    "    _messages.extend(messages)\n",
    "    _messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    \n",
    "    _log_message = \"\\n\\n============================ PROMPT ============================\\n\"\n",
    "    for message in _messages:\n",
    "        _log_message += f\"{message['role']}: {message['content']}\\n\"\n",
    "    logger.info(_log_message)\n",
    "        \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=_messages,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        frequency_penalty=frequency_penalty,\n",
    "        presence_penalty=presence_penalty,\n",
    "        stop=stop\n",
    "    )\n",
    "    _log_message = \"\\n\\n============================ RESPONSE ============================\\n\"\n",
    "    _log_message += f\"{response}\\n\"\n",
    "    logger.info(_log_message)\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.- Ejemplo de Optimización de Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.- Clase para generación de nuevas prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(instructions, problems, top_instructions=10, top_problems=5):\n",
    "    # Sort instructions by score and select top results\n",
    "    instructions = sorted(instructions, key=lambda x: x['score'], reverse=True)[:top_instructions]\n",
    "    instructions_str = \"\\n\".join([f\"{i['prompt']} (score: {i['score']})\" for i in instructions])\n",
    "    \n",
    "    # Filter top 5 problems\n",
    "    problems = problems[:top_problems]\n",
    "    \n",
    "    problems_str = \"\\n\".join([f\"Q:\\n{i['question']}\\nA:<INS>\\nGround Truth Answer:\\n{i['ground_truth']}\" for i in problems])\n",
    "    \n",
    "    prompt = f\"Your task is to generate the instruction <INS>. Below are some previous instructions with their scores. The score ranges from 0 to 100.\\n\\n{instructions_str}\\n\\nBelow are some problems.\\n{problems_str}\\n\\nGenerate an instruction that is different from all the instructions <INS> above, and has a higher score than all the instructions <INS> above. The instruction should begin with <INS> and end with </INS>. The instruction should be concise, effective, and generally applicable to all problems above.\"\n",
    "    \n",
    "    new_prompt = generate_text(prompt, model=gpt4_model, temperature=1)\n",
    "    new_prompt = new_prompt.replace(\"<INS>\", \"\").replace(\"</INS>\", \"\")\n",
    "    return new_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.- Clase para evaluación de respuestas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_answer(answer, ground_truth):\n",
    "    logger.info(\"Evaluating answer: {}\".format(answer))\n",
    "    logger.info(\"Ground Truth: {}\".format(json.dumps(ground_truth, indent=2)))\n",
    "    # load answer as json\n",
    "    try:\n",
    "        answer = json.loads(answer)\n",
    "    except:\n",
    "        logger.info(\"  answer is not valid json\")\n",
    "        return 0\n",
    "    \n",
    "    # compare each answer field with each ground truth field\n",
    "    score = 0\n",
    "    for key in answer.keys():\n",
    "        if key in ground_truth.keys():\n",
    "            if answer[key] == ground_truth[key]:\n",
    "                score += 1\n",
    "            else:\n",
    "                logger.info(\"  field '{}' does not match ground truth\".format(key))\n",
    "        else:\n",
    "            logger.info(\"  field '{}' not found in ground truth\".format(key))\n",
    "    \n",
    "    # normalize score\n",
    "    score = score / len(ground_truth.keys())\n",
    "    return score * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.- Clase para testear nuevas prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_prompt(prompt, problems, consistency_iterations=3):\n",
    "    logger.info(f\"Testing prompt with '{consistency_iterations}' consistency iterations:\\n{prompt}\", verbose=True)\n",
    "\n",
    "    all_answer_scores = []\n",
    "    problem_avg_scores = []\n",
    "\n",
    "    for i, problem in enumerate(problems):\n",
    "        full_prompt = f\"<INVOICE>\\n{problem['question']}\\n</INVOICE>\\n{prompt}\"\n",
    "        answer_scores = []\n",
    "\n",
    "        for j in range(consistency_iterations):\n",
    "            answer = generate_text(full_prompt, model=gpt35_model, temperature=0.3)\n",
    "            score = evaluate_answer(answer, problem['ground_truth'])\n",
    "            answer_scores.append(score)\n",
    "            logger.info(f\"  Problem {i+1}/{len(problems)} - Iteration {j+1}/{consistency_iterations} -> Score: {score:.3f}\", verbose=True)\n",
    "\n",
    "        problem_avg_score = np.mean(answer_scores)\n",
    "        problem_std_dev = np.std(answer_scores)\n",
    "        logger.info(f\"  Problem {i+1} Avg Score: {problem_avg_score:.3f} - Std Dev: {problem_std_dev:.3f}\", verbose=True)\n",
    "\n",
    "        problem_avg_scores.append(problem_avg_score)\n",
    "        all_answer_scores.append(answer_scores)\n",
    "\n",
    "    overall_avg_score = np.mean(problem_avg_scores)\n",
    "    overall_std_dev = np.mean([np.std(ans) for ans in all_answer_scores])\n",
    "    logger.info(f\"Overall Avg Score: {overall_avg_score:.3f} - Overall Std Dev: {overall_std_dev:.3f}\\n\", verbose=True)\n",
    "\n",
    "    return overall_avg_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.- Condiciones iniciales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIRST_PROMPT = \"\"\"You are extracting information from an <INVOICE>. Extract the following information and present it in a table in JSON format: Invoice Number, Invoice Date, Due Date, Total Amount, Tax Amount (or IVA), Recipient Name, Recipient Tax Number (or RUT), Sender Name, Sender Tax Number (or RUT). Use this format for dates: YYYY-MM-DD. Use this format for money: $1.000.000,00.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('problem_examples.json') as f:\n",
    "    problems = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.- Evaluación inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_prompt_score = test_prompt(FIRST_PROMPT, problems)\n",
    "instructions.append({\"prompt\": FIRST_PROMPT, \"score\": first_prompt_score})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.- Loop de optimización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ITERATIONS = 3\n",
    "\n",
    "for i in range(MAX_ITERATIONS):\n",
    "    logger.info(f\"======================== NEW PROMPT (ITERATION {i+1}/{MAX_ITERATIONS}) =========================\", verbose=True)\n",
    "    new_prompt = generate_prompt(instructions, problems)\n",
    "    success_score = test_prompt(new_prompt, problems)\n",
    "    instructions.append({\"prompt\": new_prompt, \"score\": success_score})\n",
    "    logger.info(\"=============================================================================\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7.- Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "df = pd.DataFrame(instructions)\n",
    "df = df.sort_values(by=['score'], ascending=False)\n",
    "df = df.reset_index(drop=True)\n",
    "df.index += 1\n",
    "df = df[['score', 'prompt']]\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
