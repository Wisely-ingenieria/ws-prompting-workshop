{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplo de código de optimizador de prompts\n",
    "Este código muestra cómo optimizar un prompt para un modelo de lenguaje GPT-3.5, utilizando GPT-4 como optimizador. El codigo esta basado en el paper [Large Language Models as Optimizers. Yang et al. (2023).](https://doi.org/10.48550/arXiv.2309.03409)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.- Setup inicial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1- Instalar librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install openai\n",
    "#! pip install tenacity\n",
    "#! pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.- Cargar librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.- Variables de entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load secrets and config from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# OpenAI API\n",
    "openai.api_type = os.getenv(\"OPENAI_API_TYPE\")\n",
    "openai.api_base = os.getenv(\"OPENAI_API_BASE\")\n",
    "openai.api_version = os.getenv(\"OPENAI_API_VERSION\")\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "embedding_model = os.getenv(\"OPENAI_EMBEDDING_MODEL\")\n",
    "print(\"OpenAI API key: {}\".format(openai.api_key[:5] + '...' + openai.api_key[-5:]))\n",
    "print(\"OpenAI API base: {}\".format(openai.api_base))\n",
    "print(\"OpenAI API version: {}\".format(openai.api_version))\n",
    "print(\"OpenAI API type: {}\".format(openai.api_type))\n",
    "\n",
    "# Model endpoint names\n",
    "gpt35_model = os.getenv(\"OPENAI_GPT35_MODEL\")\n",
    "gpt35_16k_model = os.getenv(\"OPENAI_GPT35_16K_MODEL\")\n",
    "gpt4_model = os.getenv(\"OPENAI_GPT4_MODEL\")\n",
    "print(\"GPT-3.5-Turbo model: {}\".format(gpt35_model))\n",
    "print(\"GPT-3.5-Turbo-16k model: {}\".format(gpt35_16k_model))\n",
    "print(\"GPT-4 model: {}\".format(gpt4_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.- Clase para logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger:\n",
    "    def __init__(self, log_file=None, log_dir='./logs'):\n",
    "        if not os.path.exists(log_dir):\n",
    "            os.makedirs(log_dir)\n",
    "        \n",
    "        if log_file is None:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "            log_file = f\"log_{timestamp}.log\"\n",
    "        \n",
    "        self.log_file = os.path.join(log_dir, log_file)\n",
    "\n",
    "    def _write_log(self, level, msg, verbose):\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        log_entry = f\"{timestamp} [{level}] {msg}\\n\"\n",
    "        with open(self.log_file, \"a\") as f:\n",
    "            f.write(log_entry)\n",
    "        if verbose:\n",
    "            print(log_entry.strip())\n",
    "\n",
    "    def info(self, msg, verbose=False):\n",
    "        self._write_log(\"INFO\", msg, verbose)\n",
    "\n",
    "    def warn(self, msg, verbose=False):\n",
    "        self._write_log(\"WARN\", msg, verbose)\n",
    "\n",
    "    def error(self, msg, verbose=True):\n",
    "        self._write_log(\"ERROR\", msg, verbose)\n",
    "        \n",
    "logger = Logger()\n",
    "logger.info(\"Logger initialized\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.- Clase para generación de texto con modelos GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(3))\n",
    "def generate_text(prompt, model=gpt35_model, messages=[], max_tokens=600, temperature=0.5, top_p=1.0, frequency_penalty=0.0, presence_penalty=0.0, stop=None):\n",
    "    _messages = []\n",
    "    _messages.extend(messages)\n",
    "    _messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    \n",
    "    _log_message = \"\\n\\n============================ PROMPT ============================\\n\"\n",
    "    for message in _messages:\n",
    "        _log_message += f\"{message['role']}: {message['content']}\\n\"\n",
    "    logger.info(_log_message)\n",
    "        \n",
    "    response = openai.ChatCompletion.create(\n",
    "        engine=model,\n",
    "        messages=_messages,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        frequency_penalty=frequency_penalty,\n",
    "        presence_penalty=presence_penalty,\n",
    "        stop=stop\n",
    "    )\n",
    "    _log_message = \"\\n\\n============================ RESPONSE ============================\\n\"\n",
    "    _log_message += f\"{response}\\n\"\n",
    "    logger.info(_log_message)\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.- Componentes de una prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.- Prompt simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Que es una prompt?\"\n",
    "print(generate_text(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"En una oración. Que es una prompt?\"\n",
    "print(generate_text(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Prompt: Un prompt es una señ\"\n",
    "print(generate_text(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"User: Prompt\n",
    "AI: Una prompt es una señal o indicación que se da a un usuario para realizar una acción, comúnmente usada en programación y sistemas operativos.\n",
    "User: Java\n",
    "AI:\"\"\"\n",
    "print(generate_text(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.- Mensajes previos (contexto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Prompt\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Un prompt es una señal o indicación en un programa de computadora que solicita al usuario que introduzca un comando o información.\"}\n",
    "    ]\n",
    "prompt = \"Java\"\n",
    "print(generate_text(prompt, messages=messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Prompt\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Un prompt es una señal o indicación en un programa de computadora que solicita al usuario que introduzca un comando o información.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Java\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Java es un lenguaje de programación de alto nivel, orientado a objetos y diseñado para desarrollar software para múltiples plataformas.\"}\n",
    "    ]\n",
    "prompt = \"JavaScript\"\n",
    "print(generate_text(prompt, messages=messages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.- System message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"Responde en una sola oración con una descripcion del concepto ingresado por el usuario. Responde siempre en español.\"\n",
    "messages = [{\"role\": \"system\", \"content\": system_message}]\n",
    "prompt = \"Prompt\"\n",
    "print(generate_text(prompt, messages=messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Java\"\n",
    "print(generate_text(prompt, messages=messages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.- Parametros de GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.- Stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Que es una prompt?\"\n",
    "print(generate_text(prompt, temperature=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Que es una prompt?\"\n",
    "print(generate_text(prompt, temperature=0, stop=[\".\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.- Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPERATURE_STEPS = 5\n",
    "CONSISTENCY_ITERATIONS = 3\n",
    "\n",
    "answers = []\n",
    "for i in range(TEMPERATURE_STEPS):\n",
    "    temperature = np.interp(i, [0, TEMPERATURE_STEPS-1], [0, 1.5])\n",
    "    for j in range(CONSISTENCY_ITERATIONS):\n",
    "        answer = generate_text(\"En una oracion. Que es una prompt?\", temperature=temperature)\n",
    "        print(f\"{j} - Temperature: {temperature:.2f}, Answer: {answer}\")\n",
    "        answers.append({\"temperature\": temperature, \"consistency\": j, \"answer\": answer})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.- Top P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_P_STEPS = 5\n",
    "CONSISTENCY_ITERATIONS = 3\n",
    "\n",
    "answers = []\n",
    "for i in range(TOP_P_STEPS):\n",
    "    top_p = np.interp(i, [0, TOP_P_STEPS-1], [0, 1.0])\n",
    "    for j in range(CONSISTENCY_ITERATIONS):\n",
    "        answer = generate_text(\"En una oracion. Que es una prompt?\", top_p=top_p)\n",
    "        print(f\"{j} - Top_p: {top_p:.2f}, Answer: {answer}\")\n",
    "        answers.append({\"top_p\": top_p, \"consistency\": j, \"answer\": answer})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.- Frequency penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FREQ_PENALTY_STEPS = 5\n",
    "CONSISTENCY_ITERATIONS = 3\n",
    "\n",
    "answers = []\n",
    "for i in range(FREQ_PENALTY_STEPS):\n",
    "    frequency_penalty = np.interp(i, [0, FREQ_PENALTY_STEPS-1], [0, 1.0])\n",
    "    for j in range(CONSISTENCY_ITERATIONS):\n",
    "        answer = generate_text(\"En una oracion. Que es una prompt?\", frequency_penalty=frequency_penalty)\n",
    "        print(f\"{j} - Frequency penalty: {frequency_penalty:.2f}, Answer: {answer}\")\n",
    "        answers.append({\"frequency_penalty\": frequency_penalty, \"consistency\": j, \"answer\": answer})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.- Presence penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRESENCE_PENALTY_STEPS = 5\n",
    "CONSISTENCY_ITERATIONS = 3\n",
    "\n",
    "answers = []\n",
    "for i in range(PRESENCE_PENALTY_STEPS):\n",
    "    presence_penalty = np.interp(i, [0, PRESENCE_PENALTY_STEPS-1], [0, 1.0])\n",
    "    for j in range(CONSISTENCY_ITERATIONS):\n",
    "        answer = generate_text(\"En una oracion. Que es una prompt?\", presence_penalty=presence_penalty)\n",
    "        print(f\"{j} - Presence penalty: {presence_penalty:.2f}, Answer: {answer}\")\n",
    "        answers.append({\"presence_penalty\": presence_penalty, \"consistency\": j, \"answer\": answer})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
